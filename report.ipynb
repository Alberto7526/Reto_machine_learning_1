{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "In this notebook we report progress on the project of house price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(*, model, metric, X_train, y_train, X_test, y_test):\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    train_error = metric(y_train, train_predictions)\n",
    "    test_error = metric(y_test, test_predictions)\n",
    "    return {\n",
    "        \"train_predictions\": train_predictions,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error\n",
    "    }\n",
    "\n",
    "def print_report(*, model, evaluation):\n",
    "    print(f\"Model used:\\n\\t{reg}\")\n",
    "    print(f\"Error:\\n\\ttrain set {evaluation['train_error']}\\n\\ttest error: {evaluation['test_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models\"\n",
    "dataset_path = \"./data/train_regression.csv\"\n",
    "dataset = data.get_dataset(\n",
    "    partial(pd.read_csv, filepath_or_buffer=dataset_path),\n",
    "    splits=(\"train\", \"test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you need to visualize anything from your training data, do it here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Before doing any complex Machine Learning model, let's try to solve the problem by having an initial educated guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('average-charges-extractor',\n",
      "                 AverageChargesPerRegionExtractor()),\n",
      "                ('average-charges-regressor',\n",
      "                 AverageChargesPerRegionRegressor())])\n",
      "Error:\n",
      "\ttrain set 7763.483136076173\n",
      "\ttest error: 8214.391166112087\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"Baseline\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model \n",
    "\n",
    "We want to try easy things first, so know lets see how a linear regression model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('average-charges-extractor',\n",
      "                 AverageChargesPerRegionExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 3885.0639260784033\n",
      "\ttest error: 3741.6139696012647\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-15 00-42\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort and discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with Feature Engineering\n",
    "\n",
    "Probably the previous model is not good enough, let's see how is the performance of a model using some produced features.\n",
    "\n",
    "Techniques:\n",
    "1. Feature Cross\n",
    "2. Discretizer\n",
    "3. Add average per neighborhood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('average-charges-extractor',\n",
      "                 AverageChargesPerRegionExtractor()),\n",
      "                ('discretizer',\n",
      "                 Discretizer(bins_per_column={'bmi': 4}, strategy='quantile')),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 4045.0999620912107\n",
      "\ttest error: 3983.1166212763574\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-15 01-28\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carreta asociada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
